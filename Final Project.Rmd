---
title: "Optimal song properties for popularity"
author: "Group 3"
date: "December 4th, 2024"
output: pdf_document
---

List your group members, including their student numbers, here:

-   Justin Lee (169085217)
-   Raiyan Junaid (169099497)
-   Dylan Plut (169083130)
-   Seamus Chai ()

# Introduction

Every year, Spotify users receive a summary of their year in music, nicknamed "wrapped" as it comes out before Christmas. As any artist, it can be said that appearing on Spotify users wrapped summaries is a very respectable achievement, and since many users post their wrapped data on social media, it is also good for artist growth. Furthermore, artists partner with Spotify to make videos for their top listeners (people with the most minutes streamed). For artists that did not make users wrapped's this year, they will likely be wondering what it was about their songs, if anything, that prevented them from being relevant.

# Goals

The goal of this analysis will be to compare key metrics for Spotify's songs via their API (Application Programming Interface) to see if songs with certain attributes perform better than others. To do this, we will graph various different attributes against each other and measure it relative to popularity to see correlation. If meaningful correlation is found then we will create models for the data so that future artists can optimize their songs.

# Libraries

```{r}
library(tidyverse)
library(tidymodels)
library(devtools)
library(data.table)
```

```{r}
folder_path <- "Raw Data Files"
csv_files <- list.files(path = folder_path, pattern = "\\.csv$", full.names = TRUE)
data_list <- lapply(csv_files, fread)
data_list

big_dataset <- read_csv("tracks_features.csv")
big_dataset
```

# Tidying Data

```{r}
# Joining the datasets
read_and_convert <- function(file) {
  df <- read_csv(file, col_types = cols(.default = "c"))
  df |> mutate(across(everything(), as.character))
}

all_data <- map(csv_files, read_and_convert)
combined_data <- bind_rows(all_data)

numeric_cols <- c("Duration (ms)", "Popularity", "Danceability", "Energy", "Key", "Loudness", 
                  "Mode", "Speechiness", "Acousticness", "Instrumentalness", "Liveness", 
                  "Valence", "Tempo", "Time Signature")

combined_data <- combined_data |>
  mutate(across(all_of(numeric_cols), as.numeric),
         `Release Date` = as.POSIXct(`Release Date`, format = "%Y-%m-%d")) |>
  select(Popularity, `Artist Name(s)`, `Track Name`, `Duration (ms)`, `Track ID`)



# Creating the decades column
big_set <- big_dataset |>
  mutate(release_date = as.POSIXct(release_date, format = "%Y-%m-%d")) |>
  mutate(Decade = case_when(
    year >= 2020 ~ "2020+",
    year >= 2010 & year < 2020 ~ "2010 - 2020",
    year >= 2000 & year < 2010 ~ "2000 - 2010",
    year >= 1990 & year < 2000 ~ "1990 - 2000",
    year >= 1980 & year < 1990 ~ "1980 - 1990",
    year >= 1970 & year < 1980 ~ "1970 - 1980",
    year >= 1960 & year < 1970 ~ "1960 - 1970",
    year >= 1950 & year < 1960 ~ "1950 - 1960",
    year < 1950 ~ "1950-",
  ))

# Fixing the duration column to not be in ms but instead in seconds. Also matching the "name" column for both datasets to prepare for join
combined_data <- combined_data |>
  mutate(`Duration (sec)` = `Duration (ms)` / 1000) |>
  rename(
    `artists` = `Artist Name(s)`,
    `id` = `Track ID`,
    `name` = `Track Name`
    )
#combined_data

# Fixing artists names in "big_set" to be more similar to strings instead of lists/arrays
big_set$artists <- gsub("\\[|\\]|'", "", big_set$artists)

# Matching the formatting of the new artists column in combined_data to the one in big_set.
# This line adds a space after every comma...
combined_data$artists <- gsub(",([^ ])", ", \\1", combined_data$artists) #...idek man this is built on thoughts and prayers

# Adding popularity to the big dataset from our limited dataset
complete_set <- big_set |>
  inner_join(
    combined_data,
    by = join_by(id, artists)
  ) |>
  select(-duration_ms) |> # Extra duration column from "big_set"
  # Getting rid of duplicates
  unique() |>
  arrange(artists) 

# Modifying the popularity set to be an effective factor variable set (that has ranges)
complete_set <- complete_set |>
  mutate(pop_scale = case_when(
    Popularity <= 10 ~ "0-10",
    Popularity > 10 & Popularity <= 20 ~ "11-20",
    Popularity > 20 & Popularity <= 30 ~ "21-30",
    Popularity > 30 & Popularity <= 40 ~ "31-40",
    Popularity > 40 & Popularity <= 50 ~ "41-50",
    Popularity > 50 & Popularity <= 60 ~ "51-60",
    Popularity > 60 & Popularity <= 70 ~ "61-70",
    Popularity > 70 & Popularity <= 80 ~ "71-80",
    Popularity > 80 & Popularity <= 90 ~ "81-90",
    Popularity > 90 & Popularity <= 100 ~ "91-100"
  ),
    pop_scale = as.factor(pop_scale)
    )

# Creating the artist_count column
complete_set$artist_count <- numeric(nrow(complete_set))
for (i in 1:nrow(complete_set)) {
  count <- 1  # Start with 1 because there's always at least one name
  artist_string <- complete_set$artists[i]
  
  for (char in strsplit(artist_string, "")[[1]]) {
    if (char == ",") {
      count <- count + 1
    }
  }
  complete_set$artist_count[i] <- count
}



complete_set_split <- initial_validation_split(complete_set, prop = c(0.6, 0.2), strata = Decade)
complete_set_split
```

# Visualizations

### Artist Count vs Popularity

```{r}
model <- linear_reg() |>
  set_engine("lm")

fit_artist_pop <- model |>
  fit(artist_count ~ Popularity, data = training(complete_set_split))

model_coef <- tidy(fit_artist_pop)
model_aug <- augment(fit_artist_pop, new_data = training(complete_set_split))


recipe_artist_pop <- recipe(pop_scale ~ artist_count + Popularity + Decade, data = training(complete_set_split)) |>
  #step_log(artist_count, offset = 1) |>
  #step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

baked_data <- bake(prep(recipe_artist_pop), new_data = training(complete_set_split)) |>
  pivot_longer(
    cols = starts_with("Decade_X"),
    names_to = "Decade",
    values_to = "which_dec",
    values_drop_na = TRUE
  ) |>
  filter(which_dec == 1) |>
  select(-which_dec)

# TEST MODEL
art_test <- ggplot(baked_data, aes(x = artist_count, y = pop_scale, color = Decade)) +
  geom_count(alpha = 0.6) +
  #geom_smooth(method = "lm", se = FALSE) +
  scale_x_log10() +
  theme_minimal() +
  labs(
    title = "Relationship between Artist Count and Scaled Popularity",
    subtitle = "Comparison across decades",
    x = "Artist Count",
    y = "Scaled Popularity",
    color = "Decade"
  )
art_test 


# MAIN MODEL
artists_vs_popularity <- model_aug |>
  ggplot(aes(x = artist_count, y = .pred)) +
  geom_point(aes(colour = Decade)) + 
  facet_wrap(~Decade) +
  #geom_abline(intercept = 1.6530366571, slope = 0.01) + <- HAS SOME PROMISE BUT IDK HOW TO USE EFFECTIVELY HERE
  geom_smooth(aes(colour = Decade), method = "lm", se = FALSE) +
  #geom_smooth(color = "black", method = "loess", se = FALSE) +
  theme_minimal() #+
  #labs(
  #  x = "Artist Count",
  #  y = "Recent Popularity",
  #  title = "How much does the number of artists affect popularity?",
  #  subtitle = "Split by decade"
  #)

artists_vs_popularity
```

### Acousticness vs Tempo and Explicity

```{r}
Acous_vs_temp_plot <- training(complete_set_split) |> ggplot(aes(y = acousticness, x = tempo)) +
  geom_point(aes(colour = time_signature)) +
  geom_smooth(se = FALSE) +
  facet_wrap(~explicit)

Acous_vs_temp_plot
```

The graph above showcases how different song compositions tend to have a more general pattern, as shown by the graph when the song is not explicit there is an inverse correlation between the acousticness and the tempo of the song. This differs from the graph of the songs which are explicit as the slope is almost flat with there being close to no correlation. With this we can imply the composition of a song based on the factors of tempo and the explicitness. It is important to note that with these conclusions they can be biased based on the small sample size. (see limitations)

```{r}
plot_year_histogram <- training(complete_set_split) |> 
  ggplot(aes(x = year, fill = after_stat(count))) + 
  geom_histogram( color = "black") +
  scale_fill_viridis_c() +
  labs(title = "Distribution of Years",
       x = "Year",
       y = "Count",
       fill = "Frequency") +
  theme_minimal()

plot_year_histogram
```

The graph shown above demonstrates one of the measure we took to circumvent the downfalls of our data. This problem we had to solve was that the popularity value found on the Spotify api is more a score of relevancy with parts of its calculations being based on recent listens, this posed difficulty with songs as an older mor popular song would have a lower popularity score. To circumvent this issue we decided to pull the majority of data based in recent years as shown by the graph. This also caused some issue but that is also found in the limitations.

```{r}
plot_bar <- training(complete_set_split) |> 
  ggplot(aes(x = Decade, group = key, fill = key)) + 
  geom_bar(position = "dodge") +
  labs(title = "Song Count by Decade and Musical Key",
       x = "Decade",
       y = "Count",
       fill = "Musical Key") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

plot_bar

```

The bar graph above showcases how the musical key of our data changes throughout the decades, this could suggest changes in the differences of the musical taste. This data may suggest that but the main difficulty with this interpretation is the affect of the small sample size therefore any conclusions must be taken with a grain of salt.

The future possibility of this study could further dive into the impacts of society of music with certain keys potentially being more popular in certain decades.

For whoever is writing insights (From Raiyan):

-   Not enough data to accurately represent other regions

-   If you run without facet_wrap(\~Decade) 2010-2020 fully influences the entire graph. It is basically the whole graph because of the lack of any actual data points after splitting

-   If we want to pull real information from this graph, we need one of the following to happen:

    -   Far more information with popularity numbers for things in our dataset

    -   Me to only focus on 2010-2020 (which at that point isn't even recent so it may reduce credibility)

    -   Write a massive paragraph about what happened and how "there was nothing we could do" (even though there is the other options I said above)

-   Only good option is to pull more information but without popularity values this becomes a little tough to try and argue (but if you think you can do it then go ahead)

Send in discord what we decide so everyone is on the same page.

**UPDATES:**

-   I modified the join properties to allow more data to come through, I'm not sure how it changes the quality of it but now we only sort by id/artist, no longer name. This lets us get a little bit of new information out...but its like 4 entries sooooo

-   If we only sort by name/artist, I think its allowing duplicate entries (even though we isolated those earlier) so something funky might be happening. If we get desperate then we can use that though and that doubles our available data. Just a little bit dishonest since we are now intentionally skewing data

# Limitations

Throughout this project, we faced a very large number of technical challenges, which
