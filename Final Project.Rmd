---
title: "Optimal song properties for popularity"
author: "Group 3"
date: "December 4th, 2024"
output: pdf_document
---

List your group members, including their student numbers, here:

-   Justin Lee (169085217)
-   Raiyan Junaid (169099497)
-   Dylan Plut ()
-   Seamus Chai ()

# Libraries

```{r}
library(tidyverse)
library(tidymodels)
library(devtools)
library(data.table)
```

```{r}
folder_path <- "Raw Data Files"
csv_files <- list.files(path = folder_path, pattern = "\\.csv$", full.names = TRUE)
data_list <- lapply(csv_files, fread)
data_list

big_dataset <- read_csv("tracks_features.csv")
big_dataset
```

# Tidying Data

```{r}
# Joining the datasets
read_and_convert <- function(file) {
  df <- read_csv(file, col_types = cols(.default = "c"))
  df |> mutate(across(everything(), as.character))
}

all_data <- map(csv_files, read_and_convert)
combined_data <- bind_rows(all_data)

numeric_cols <- c("Duration (ms)", "Popularity", "Danceability", "Energy", "Key", "Loudness", 
                  "Mode", "Speechiness", "Acousticness", "Instrumentalness", "Liveness", 
                  "Valence", "Tempo", "Time Signature")

combined_data <- combined_data |>
  mutate(across(all_of(numeric_cols), as.numeric),
         `Release Date` = as.POSIXct(`Release Date`, format = "%Y-%m-%d")) |>
  select(Popularity, `Artist Name(s)`, `Track Name`, `Duration (ms)`, `Track ID`)



# Creating the decades column
big_set <- big_dataset |>
  mutate(release_date = as.POSIXct(release_date, format = "%Y-%m-%d")) |>
  mutate(Decade = case_when(
    year >= 2020 ~ "2020+",
    year >= 2010 & year < 2020 ~ "2010 - 2020",
    year >= 2000 & year < 2010 ~ "2000 - 2010",
    year >= 1990 & year < 2000 ~ "1990 - 2000",
    year >= 1980 & year < 1990 ~ "1980 - 1990",
    year >= 1970 & year < 1980 ~ "1970 - 1980",
    year >= 1960 & year < 1970 ~ "1960 - 1970",
    year >= 1950 & year < 1960 ~ "1950 - 1960",
    year < 1950 ~ "1950-",
  ))

# Fixing the duration column to not be in ms but instead in seconds. Also matching the "name" column for both datasets to prepare for join
combined_data <- combined_data |>
  mutate(`Duration (sec)` = `Duration (ms)` / 1000) |>
  rename(
    `artists` = `Artist Name(s)`,
    `id` = `Track ID`,
    `name` = `Track Name`
    )
#combined_data

# Fixing artists names in "big_set" to be more similar to strings instead of lists/arrays
big_set$artists <- gsub("\\[|\\]|'", "", big_set$artists)

# Matching the formatting of the new artists column in combined_data to the one in big_set.
# This line adds a space after every comma...
combined_data$artists <- gsub(",([^ ])", ", \\1", combined_data$artists) #...idek man this is built on thoughts and prayers

# Adding popularity to the big dataset from our limited dataset
complete_set <- big_set |>
  inner_join(
    combined_data,
    by = join_by(id, artists)
  ) |>
  select(-duration_ms) |> # Extra duration column from "big_set"
  # Getting rid of duplicates
  unique() |>
  arrange(artists) 

# Modifying the popularity set to be an effective factor variable set (that has ranges)
complete_set <- complete_set |>
  mutate(pop_scale = case_when(
    Popularity <= 10 ~ "0-10",
    Popularity > 10 & Popularity <= 20 ~ "11-20",
    Popularity > 20 & Popularity <= 30 ~ "21-30",
    Popularity > 30 & Popularity <= 40 ~ "31-40",
    Popularity > 40 & Popularity <= 50 ~ "41-50",
    Popularity > 50 & Popularity <= 60 ~ "51-60",
    Popularity > 60 & Popularity <= 70 ~ "61-70",
    Popularity > 70 & Popularity <= 80 ~ "71-80",
    Popularity > 80 & Popularity <= 90 ~ "81-90",
    Popularity > 90 & Popularity <= 100 ~ "91-100"
  ),
    pop_scale = as.factor(pop_scale)
    )

# Creating the artist_count column
complete_set$artist_count <- numeric(nrow(complete_set))
for (i in 1:nrow(complete_set)) {
  count <- 1  # Start with 1 because there's always at least one name
  artist_string <- complete_set$artists[i]
  
  for (char in strsplit(artist_string, "")[[1]]) {
    if (char == ",") {
      count <- count + 1
    }
  }
  complete_set$artist_count[i] <- count
}



complete_set_split <- initial_validation_split(complete_set, prop = c(0.6, 0.2), strata = Decade)
complete_set_split
```

# Visualizations

### Artist Count vs Popularity

```{r}
model <- linear_reg() |>
  set_engine("lm")

fit_artist_pop <- model |>
  fit(artist_count ~ Popularity, data = training(complete_set_split))

model_coef <- tidy(fit_model)
model_aug <- augment(fit_model, new_data = training(complete_set_split))


recipe_artist_pop <- recipe(pop_scale ~ artist_count + Popularity + Decade, data = training(complete_set_split)) |>
  #step_log(artist_count, offset = 1) |>
  #step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

baked_data <- bake(prep(recipe_artist_pop), new_data = training(complete_set_split)) |>
  pivot_longer(
    cols = starts_with("Decade_X"),
    names_to = "Decade",
    values_to = "which_dec",
    values_drop_na = TRUE
  ) |>
  filter(which_dec == 1) |>
  select(-which_dec)

# TEST MODEL
art_test <- ggplot(baked_data, aes(x = artist_count, y = pop_scale, color = Decade)) +
  geom_count(alpha = 0.6) +
  #geom_smooth(method = "lm", se = FALSE) +
  scale_x_log10() +
  theme_minimal() +
  labs(
    title = "Relationship between Artist Count and Scaled Popularity",
    subtitle = "Comparison across decades",
    x = "Artist Count",
    y = "Scaled Popularity",
    color = "Decade"
  )
art_test 


# MAIN MODEL
artists_vs_popularity <- model_aug |>
  ggplot(aes(x = artist_count, y = .pred)) +
  geom_point(aes(colour = Decade)) + 
  facet_wrap(~Decade) +
  #geom_abline(intercept = 1.6530366571, slope = 0.01) + <- HAS SOME PROMISE BUT IDK HOW TO USE EFFECTIVELY HERE
  geom_smooth(aes(colour = Decade), method = "lm", se = FALSE) +
  #geom_smooth(color = "black", method = "loess", se = FALSE) +
  theme_minimal() #+
  #labs(
  #  x = "Artist Count",
  #  y = "Recent Popularity",
  #  title = "How much does the number of artists affect popularity?",
  #  subtitle = "Split by decade"
  #)

artists_vs_popularity
```

For whoever is writing insights (From Raiyan):

-   Not enough data to accurately represent other regions

-   If you run without facet_wrap(\~Decade) 2010-2020 fully influences the entire graph. It is basically the whole graph because of the lack of any actual data points after splitting

-   If we want to pull real information from this graph, we need one of the following to happen:

    -   Far more information with popularity numbers for things in our dataset

    -   Me to only focus on 2010-2020 (which at that point isn't even recent so it may reduce credibility)

    -   Write a massive paragraph about what happened and how "there was nothing we could do" (even though there is the other options I said above)

-   Only good option is to pull more information but without popularity values this becomes a little tough to try and argue (but if you think you can do it then go ahead)

Send in discord what we decide so everyone is on the same page.

**UPDATES:**

-   I modified the join properties to allow more data to come through, I'm not sure how it changes the quality of it but now we only sort by id/artist, no longer name. This lets us get a little bit of new information out...but its like 4 entries sooooo

-   If we only sort by name/artist, I think its allowing duplicate entries (even though we isolated those earlier) so something funky might be happening. If we get desperate then we can use that though and that doubles our available data. Just a little bit dishonest since we are now intentionally skewing data
