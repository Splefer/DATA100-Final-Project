---
title: "Optimal song properties for popularity"
author: "Group 3"
date: "December 4th, 2024"
output: pdf_document
---

List your group members, including their student numbers, here:

-   Justin Lee (169085217)
-   Raiyan Junaid (169099497)
-   Dylan Plut (169083130)
-   Seamus Chai ()

# Introduction

Every year, Spotify users receive a summary of their year in music, nicknamed "wrapped" as it comes out before Christmas. As any artist, it can be said that appearing on Spotify users wrapped summaries is a very respectable achievement, and since many users post their wrapped data on social media, it is also good for artist growth. Furthermore, artists partner with Spotify to make videos for their top listeners (people with the most minutes streamed). For artists that did not make users wrapped's this year, they will likely be wondering what it was about their songs, if anything, that prevented them from being relevant.

# Goals

The goal of this analysis will be to compare key metrics for Spotify's songs via their API (Application Programming Interface) to see if songs with certain attributes perform better than others. To do this, we will graph various different attributes against each other and measure it relative to popularity to see correlation. If meaningful correlation is found then we will create models for the data so that future artists can optimize their songs.

# Libraries

```{r}
library(tidyverse)
library(tidymodels)
library(devtools)
library(ggrepel)
#library(ggraph)
#library(igraph)
library(neuralnet)
library(ranger)
library(data.table)
```

```{r}
folder_path <- "Raw Data Files"
csv_files <- list.files(path = folder_path, pattern = "\\.csv$", full.names = TRUE)
data_list <- lapply(csv_files, fread)
data_list

big_dataset <- read_csv("tracks_features.csv")
big_dataset
```

# Tidying Data

```{r}
# Joining the datasets
read_and_convert <- function(file) {
  df <- read_csv(file, col_types = cols(.default = "c"))
  df |> mutate(across(everything(), as.character))
}

all_data <- map(csv_files, read_and_convert)
combined_data <- bind_rows(all_data)

numeric_cols <- c("Duration (ms)", "Popularity", "Danceability", "Energy", "Key", "Loudness", 
                  "Mode", "Speechiness", "Acousticness", "Instrumentalness", "Liveness", 
                  "Valence", "Tempo", "Time Signature")

combined_data <- combined_data |>
  mutate(across(all_of(numeric_cols), as.numeric),
         `Release Date` = as.POSIXct(`Release Date`, format = "%Y-%m-%d")) |>
  select(Popularity, `Artist Name(s)`, `Track Name`, `Duration (ms)`, `Track ID`)



# Creating the decades column
big_set <- big_dataset |>
  mutate(release_date = as.POSIXct(release_date, format = "%Y-%m-%d")) |>
  mutate(Decade = case_when(
    year >= 2020 ~ "2020+",
    year >= 2010 & year < 2020 ~ "2010 - 2020",
    year >= 2000 & year < 2010 ~ "2000 - 2010",
    year >= 1990 & year < 2000 ~ "1990 - 2000",
    year >= 1980 & year < 1990 ~ "1980 - 1990",
    year >= 1970 & year < 1980 ~ "1970 - 1980",
    year >= 1960 & year < 1970 ~ "1960 - 1970",
    year >= 1950 & year < 1960 ~ "1950 - 1960",
    year < 1950 ~ "1950-",
  ))

# Fixing the duration column to not be in ms but instead in seconds. Also matching the "name" column for both datasets to prepare for join
combined_data <- combined_data |>
  mutate(`Duration (sec)` = `Duration (ms)` / 1000) |>
  rename(
    `artists` = `Artist Name(s)`,
    `id` = `Track ID`,
    `name` = `Track Name`
    )
#combined_data

# Fixing artists names in "big_set" to be more similar to strings instead of lists/arrays
big_set$artists <- gsub("\\[|\\]|'", "", big_set$artists)

# Matching the formatting of the new artists column in combined_data to the one in big_set.
# This line adds a space after every comma...
combined_data$artists <- gsub(",([^ ])", ", \\1", combined_data$artists) #...idek man this is built on thoughts and prayers

# Adding popularity to the big dataset from our limited dataset
complete_set <- big_set |>
  inner_join(
    combined_data,
    by = join_by(id, artists)
  ) |>
  select(-duration_ms) |> # Extra duration column from "big_set"
  # Getting rid of duplicates
  unique() |>
  arrange(artists) 

# Modifying the popularity set to be an effective factor variable set (that has ranges)
complete_set <- complete_set |>
  mutate(pop_scale = case_when(
    Popularity <= 10 ~ "0-10",
    Popularity > 10 & Popularity <= 20 ~ "11-20",
    Popularity > 20 & Popularity <= 30 ~ "21-30",
    Popularity > 30 & Popularity <= 40 ~ "31-40",
    Popularity > 40 & Popularity <= 50 ~ "41-50",
    Popularity > 50 & Popularity <= 60 ~ "51-60",
    Popularity > 60 & Popularity <= 70 ~ "61-70",
    Popularity > 70 & Popularity <= 80 ~ "71-80",
    Popularity > 80 & Popularity <= 90 ~ "81-90",
    Popularity > 90 & Popularity <= 100 ~ "91-100"
  ),
    pop_scale = as.factor(pop_scale)
    )

# Creating the artist_count column
complete_set$artist_count <- numeric(nrow(complete_set))
for (i in 1:nrow(complete_set)) {
  count <- 1  # Start with 1 because there's always at least one name
  artist_string <- complete_set$artists[i]
  
  for (char in strsplit(artist_string, "")[[1]]) {
    if (char == ",") {
      count <- count + 1
    }
  }
  complete_set$artist_count[i] <- count
}



complete_set_split <- initial_validation_split(complete_set, prop = c(0.6, 0.2), strata = Decade)
complete_set_split
```

# Visualizations

### Artist Count vs Popularity

```{r}
model <- linear_reg() |>
  set_engine("lm")

fit_artist_pop <- model |>
  fit(artist_count ~ pop_scale, data = training(complete_set_split))

model_coef <- tidy(fit_artist_pop)
model_aug <- augment(fit_artist_pop, new_data = training(complete_set_split))


recipe_artist_pop <- recipe(pop_scale ~ artist_count + Popularity + Decade, data = training(complete_set_split)) |>
  #step_log(artist_count, offset = 1) |>
  #step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

baked_data <- bake(prep(recipe_artist_pop), new_data = training(complete_set_split)) |>
  pivot_longer(
    cols = starts_with("Decade_X"),
    names_to = "Decade",
    values_to = "which_dec",
    values_drop_na = TRUE
  ) |>
  filter(which_dec == 1) |>
  select(-which_dec)

# TEST MODEL
art_test <- ggplot(baked_data, aes(x = artist_count, y = pop_scale, color = Decade)) +
  geom_count(alpha = 0.6) +
  #geom_smooth(method = "lm", se = FALSE) +
  scale_x_log10() +
  theme_minimal() +
  labs(
    title = "Relationship between Artist Count and Scaled Popularity",
    subtitle = "Comparison across decades",
    x = "Artist Count",
    y = "Scaled Popularity",
    color = "Decade"
  )
art_test 


# MAIN MODEL
artists_vs_popularity <- model_aug |>
  ggplot(aes(x = artist_count, y = .pred)) +
  geom_point(aes(colour = Decade)) + 
  facet_wrap(~Decade) +
  #geom_abline(intercept = 1.6530366571, slope = 0.01) + <- HAS SOME PROMISE BUT IDK HOW TO USE EFFECTIVELY HERE
  geom_smooth(aes(colour = Decade), method = "lm", se = FALSE) +
  #geom_smooth(color = "black", method = "loess", se = FALSE) +
  theme_minimal() #+
  #labs(
  #  x = "Artist Count",
  #  y = "Recent Popularity",
  #  title = "How much does the number of artists affect popularity?",
  #  subtitle = "Split by decade"
  #)

artists_vs_popularity
```

### Exploratory Plots - Acousticness vs Tempo and Explicity

```{r}
Acous_vs_temp_plot <- training(complete_set_split) |> ggplot(aes(y = acousticness, x = tempo)) +
  geom_point(aes(colour = time_signature)) +
  geom_smooth(se = FALSE) +
  facet_wrap(~explicit)

Acous_vs_temp_plot
```

```{r}
view(training(complete_set_split))
complete_set_split
```

```{r}
plot_year_histogram <- training(complete_set_split) |> ggplot(aes(x = year)) + geom_histogram()

plot_year_histogram
```

```{r}
plot_bar <- training(complete_set_split) |> ggplot() + 
  geom_bar(aes(x=key, y= Popularity), stat="identity")

plot_bar
```

### Exploratory Linear Models

```{r}
set.seed(1234)
valid_set <- vfold_cv(training(complete_set_split), v = 5)

model <- linear_reg() |>
  set_engine("lm")

fit_artist_pop <- model |>
  fit(artist_count ~ pop_scale, data = training(complete_set_split))

model_coef <- tidy(fit_artist_pop)
model_aug <- augment(fit_artist_pop, new_data = training(complete_set_split))

recipe_artist_pop <- recipe(artist_count ~ Popularity + Decade, data = training(complete_set_split)) |>
  step_dummy(all_nominal_predictors())


artists_vs_popularity <- model_aug |>
  ggplot(aes(x = artist_count, y = .pred)) +
  geom_point(aes(colour = Decade)) + 
  facet_wrap(~Decade) +
  #geom_abline(intercept = 1.6530366571, slope = 0.01) + <- HAS SOME PROMISE BUT IDK HOW TO USE EFFECTIVELY HERE
  geom_smooth(aes(colour = Decade), method = "lm", se = FALSE) +
  #geom_smooth(color = "black", method = "loess", se = FALSE) +
  theme_minimal() +
  labs(
    x = "Artist Count",
    y = "Predicted Popularity",
    title = "How much does the number of artists affect popularity?",
    subtitle = "Split by decade"
  )

artists_vs_popularity
```

### Exploratory Linear Model 2 - Neural Net/MLP

```{r}
model_data <- model.matrix(~ Popularity + Decade - 1, data = training(complete_set_split))
model_data <- cbind(model_data, artist_count = training(complete_set_split)$artist_count)

scaled_data <- as.data.frame(scale(model_data))
colnames(scaled_data) <- make.names(colnames(scaled_data))

formula <- artist_count ~ Popularity + Decade1990...2000 + Decade2000...2010 + Decade2010...2020 + Decade2020.
mlp_model <- neuralnet(formula, 
                       data = scaled_data, 
                       hidden = c(5, 3), 
                       linear.output = TRUE)

mlp_predictions <- predict(mlp_model, newdata = scaled_data)

plot(mlp_model)

rmse <- sqrt(mean((mlp_predictions - scaled_data$artist_count)^2))
print(paste("RMSE:", rmse))
```

### Final Model Comparison

```{r}
set.seed(1234)
valid_set <- vfold_cv(training(complete_set_split), v = 5)

model <- linear_reg() |>
  set_engine("lm")

fit_artist_pop <- model |>
  fit(artist_count ~ pop_scale, data = training(complete_set_split))

model_coef <- tidy(fit_artist_pop)
model_aug <- augment(fit_artist_pop, new_data = training(complete_set_split))

recipe_artist_pop <- recipe(artist_count ~ Popularity + Decade, data = training(complete_set_split)) |>
  step_dummy(all_nominal_predictors())

art_pop_workflow_set <- workflow_set(
  preproc = list(recipe_artist_pop),
  models = list(lm = model)
)

train_test <- initial_split(complete_set, prop = 0.8)

lm_versus_rf <- workflow_set(
    preproc = list(recipe = recipe_artist_pop), 
    models = list(lm = lm_model, rf = rf)
) |>
    workflow_map(
        fn = "tune_grid",
        grid = tune_grid,
        seed = 1234,
        control = control_grid(save_pred = TRUE, save_workflow = TRUE),
        resamples = vfold_cv(training(train_test), v = 5) 
    )

lm_versus_rf |>
    autoplot(select_best = TRUE, metric = "rmse") +
    geom_label_repel(aes(label = wflow_id))


best_lm <- lm_versus_rf |>
  extract_workflow("recipe_lm") |>
  finalize_workflow(
    lm_versus_rf |>
      extract_workflow_set_result("recipe_lm") |>
      select_best(metric = "rmse")
  )

best_rf <- lm_versus_rf |>
  extract_workflow("recipe_rf") |>
  finalize_workflow(
    lm_versus_rf |>
      extract_workflow_set_result("recipe_rf") |>
      select_best(metric = "rmse")
  )


# Copied from Assignment 4
test_lm <- last_fit(best_lm, split = complete_set_split)
test_rf <- last_fit(best_rf, split = complete_set_split)

cat("\nBest RMSE and R^2:\n")
bind_rows(
    lm = collect_metrics(test_lm),
    rf = collect_metrics(test_rf),
    .id = "model"
)

cat("\nPredictors used:\n")
extract_recipe(test_lm) |> formula()
extract_recipe(test_rf) |> formula()

bind_rows(
    lm = collect_predictions(test_lm),
    rf = collect_predictions(test_rf),
    .id = "model"
) |>
    ggplot() +
    aes(x = artist_count, y = .pred, colour = model) +
    geom_point(shape = 1)

bind_rows(
    lm = test_lm,
    rf = test_rf,
    .id = "model"
) |>
    unnest_wider(.metrics) |>
    unnest_longer(c(.metric, .estimator, .estimate, .config)) |>
    ggplot() +
    aes(x = model, y = .estimate) +
    geom_col(fill = "lightgrey", color = 1) +
    facet_wrap(~ .metric, scales = "free_y")
```

```{r}
set.seed(1234)

# Prepare data
model_data <- model.matrix(~ Popularity + Decade - 1, data = training(complete_set_split))
model_data <- cbind(model_data, artist_count = training(complete_set_split)$artist_count)
scaled_data <- as.data.frame(scale(model_data))
colnames(scaled_data) <- make.names(colnames(scaled_data))

# Check column names
print(colnames(scaled_data))

# Define formula based on actual column names
formula <- artist_count ~ Popularity + Decade1990...2000 + Decade2000...2010 + Decade2010...2020 + Decade2020.

# MLP model
mlp_model <- neuralnet(formula, 
                       data = scaled_data, 
                       hidden = c(5, 3), 
                       linear.output = TRUE)
mlp_predictions <- predict(mlp_model, newdata = scaled_data)
mlp_rmse <- sqrt(mean((mlp_predictions - scaled_data$artist_count)^2))








lm_predictions <- predict(lm_fit, new_data = scaled_data)
lm_rmse <- sqrt(mean((lm_predictions$.pred - scaled_data$artist_count)^2))

# Compare RMSE
rmse_comparison <- data.frame(
    Model = c("MLP", "RF", "LM"),
    RMSE = c(mlp_rmse, rf_rmse, lm_rmse)
)

print(rmse_comparison)

# Visualize predictions
predictions_df <- data.frame(
    Actual = scaled_data$artist_count,
    MLP = as.vector(mlp_predictions),
    RF = rf_predictions$.pred,
    LM = lm_predictions$.pred
)

predictions_long <- pivot_longer(predictions_df, cols = c(MLP, RF, LM), names_to = "Model", values_to = "Predicted")

ggplot(predictions_long, aes(x = Actual, y = Predicted, color = Model)) +
    geom_point(alpha = 0.5) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    facet_wrap(~ Model) +
    labs(title = "Actual vs Predicted Artist Count",
         x = "Actual Artist Count",
         y = "Predicted Artist Count") +
    theme_minimal()

```

```{r}
set.seed(1234)

# Prepare data
model_data <- model.matrix(~ Popularity + Decade - 1, data = training(complete_set_split))
model_data <- cbind(model_data, artist_count = training(complete_set_split)$artist_count)
scaled_data <- as.data.frame(scale(model_data))
colnames(scaled_data) <- make.names(colnames(scaled_data))

# Define formula based on actual column names
formula <- artist_count ~ Popularity + Decade1990...2000 + Decade2000...2010 + Decade2010...2020 + Decade2020.

# Define MLP model specification
mlp_model_spec <- mlp(hidden_units = c(5, 3)) %>%
    set_engine("nnet") %>%
    set_mode("regression")

# Define linear regression model specification
lm_model_spec <- linear_reg() %>%
    set_engine("lm")

# Create recipe for preprocessing
recipe_artist_pop <- recipe(formula, data = scaled_data)

# Split the data
train_test <- initial_split(scaled_data, prop = 0.8)

# Create workflow set with both models
lm_versus_mlp <- workflow_set(
    preproc = list(recipe = recipe_artist_pop), 
    models = list(lm = lm_model_spec, mlp = mlp_model_spec)
) |>
    workflow_map(
        fn = "tune_grid",
        grid = tune_grid,
        seed = 1234,
        control = control_grid(save_pred = TRUE, save_workflow = TRUE),
        resamples = vfold_cv(training(train_test), v = 5) 
    )

# Plot results
lm_versus_mlp |>
    autoplot(select_best = TRUE, metric = "rmse") +
    geom_label_repel(aes(label = wflow_id))

# Extract best workflows
best_lm <- lm_versus_mlp |>
  extract_workflow("recipe_lm") |>
  finalize_workflow(
    lm_versus_mlp |>
      extract_workflow_set_result("recipe_lm") |>
      select_best(metric = "rmse")
  )

best_mlp <- lm_versus_mlp |>
  extract_workflow("recipe_mlp") |>
  finalize_workflow(
    lm_versus_mlp |>
      extract_workflow_set_result("recipe_mlp") |>
      select_best(metric
(~ .metric, scales = "free_y")
```

# Limitations

Throughout this project, we faced a very large number of technical challenges, which
